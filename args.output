if not input_path.exists():
        logger.error("Input file not found: %s", input_path)
        return 1

    logger.info("Loading URLs from %s", input_path)
    raw_urls = load_json_file(input_path)
    urls = normalize_input_urls(raw_urls)

    all_records: List[Dict[str, Any]] = []
    for entry in urls:
        url = entry["url"]
        meta = entry.get("meta", {})
        logger.info("Processing URL: %s", url)
        records = process_url(url, settings, meta=meta)
        all_records.extend(records)

    if not all_records:
        logger.warning("No records extracted from input URLs.")

    # Decide output format
    if args.format:
        fmt = args.format.lower()
    else:
        # From settings or inferred from extension
        fmt = (
            settings.get("output", {})
            .get("default_format", "json")
            .lower()
        )
        if fmt not in {"json", "csv", "xlsx", "xml", "html"}:
            # Infer from extension
            ext = output_path.suffix.lower().lstrip(".")
            fmt = ext if ext in {"json", "csv", "xlsx", "xml", "html"} else "json"

    logger.info("Exporting %s records to %s as %s", len(all_records), output_path, fmt)
    export_results(all_records, output_path, fmt)
    logger.info("Done.")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())